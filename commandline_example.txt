#フルパラ
python3 train.py \
    --model_name "llm-jp/llm-jp-3-13b" \
    --dataset_name "your-dataset" \
    --output_dir "./full-ft-output" \
    --training_mode "full" \
    --batch_size 2 \
    --gradient_accumulation_steps 32 \
    --learning_rate 1e-5 \
    --num_epochs 2 \
    --bf16 \
    --gradient_checkpointing \
    --deepspeed_config "./configs/deepspeed_config.json"


#LoRA
python3 train.py \
    --model_name "llm-jp/llm-jp-3-13b" \
    --dataset_name "DeL-TaiseiOzaki/ichikara_003_all" \
    --output_dir "./lora-ft-output" \
    --training_mode "lora" \
    --batch_size 1 \
    --learning_rate 1e-4 \
    --num_epochs 2 \
    --fp16 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules "q_proj" "v_proj" "k_proj" "o_proj"