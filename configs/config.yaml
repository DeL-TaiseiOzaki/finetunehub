model:
  name_or_path: "google/gemma-2-9b"  # 使用するモデル名またはパス
  tokenizer_name: null  # トークナイザ名（nullの場合、モデル名を使用）
  cache_dir: null       # モデルキャッシュディレクトリ（必要に応じて）

dataset:
  load_from: "huggingface"  # "huggingface" または "local" を指定
  name:   "MKJ-TOE/detect_missinfo_instruction_ja"  # Hugging Face データセット名
  data_files:
    train: "data/dataset_train.json"  # ローカルデータファイルのパス
  cache_dir: null  # 必要に応じてデータセットキャッシュディレクトリを指定

training:
  output_dir: "./output/sft_01"  # ファインチューニング後のモデルを保存するディレクトリ
  num_train_epochs: 1
  per_device_train_batch_size: 2 #メモリエラーになる場合はここ下げると良い．時間増えます．
  block_size: 512
  learning_rate: 1e-5
  seed: 42
  fp16: false
  bf16: true
  optim: "adamw_torch"
  gradient_checkpointing: false
  ddp: false  #マルチGPUを使用する場合は true

lora:
  use_lora: false        # LoRAを使用する場合は true
  use_qlora: false       # QLoRAを使用する場合は true
  r: 8
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "v_proj"]  # LoRAを適用するモジュール #("q_proj" "k_proj" "v_proj" "o_proj" "gate_proj" "up_proj" "down_proj")

prompts:
  use_custom_prompt: true
  use_chat_template: true 
  default_system_prompt: "あなたは誠実で優秀な日本人のアシスタントです。特に指示が無い場合は、常に日本語で回答してください。\n"
  user_prompt: "タスクが曖昧または不明確であると判断した場合、効果的に進行するために必要な不足している情報や不明確な点を特定し、具体的に示してください。\n\nユーザーのタスク:"

logging:
  logging_dir: "./logs"
  logging_steps: 500
  use_wandb: false  # wandbを使用する場合は true に設定

